<!DOCTYPE html>
<html>
<head>
    <title>wgertler | Tumbling Down the Rabbit Hole</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="William Gertler">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="/css/kube.css" />
    <link rel="stylesheet" href="/css/font-awesome.css" />
    <link rel="stylesheet" href="/css/custom.css" />
    <link rel="icon"       href="/img/favicon.png" />
    <link href="http://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet" type="text/css">

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/javascript" async src="/MathJax-master/MathJax.js?config=TeX-AMS_CHTML-full"></script>
<script type="text/javascript" src="/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" src="/MathJax-master/braket/braket.js"></script>

</head>
<body>
	<!-- Navigation -->
	<div class="main-nav">
		<div class="container">
			<header class="group top-nav">
				<nav class="navbar logo-w navbar-left" >
					<a class="logo" href="/">wgertler</a>
				</nav>
				<div class="navigation-toggle" data-tools="navigation-toggle" data-target="#navbar-1">
				    <span class="logo">wgertler</span>
				</div>
			    <nav id="navbar-1" class="navbar item-nav navbar-right">
				    <ul>
				        <li><a href="/">Home</a></li>
				        <li><a href="/about/">About</a></li>
				        <li><a href="/articles/">Articles</a></li>
				        <li><a href="/contact/">Contact</a></li>
				    </ul>
				</nav>
			</header>
		</div>
	</div>

	<!-- Content -->
<div class="content">
    <div class="container">
        <!-- Post -->
        <div class="post">
            <!-- Heading -->
                <h1>Part II - Information Theory</h1>
                <h4>August 2019</h4>
                <a href="/articles/bht/classtherm">Part I</a>
                <span style="float:right;">
                <a href="/articles/bht/geometry-1">Part III</a>
                </span>
            <hr>
            <div class="in-content">
                <div class="quote">
                    <p>
                        "Where is the life we have lost in living?<br>
                        Where is the wisdom we have lost in knowledge?<br>
                        Where is the knowledge we have lost in information?"<br>
                        - T.S. Eliot, "The Rock"
                    </p>
                </div>

                <h5>Index</h5>

                <ol type='i'>
                    <li><a href="#intro">Introduction & Motivation</a></li>
                    <li><a href="#capacity">Channel Capacity</a></li>
                    <li><a href="#markov">Markov and Ergodic Processes</a></li> 
                    <li><a href="#entropy">Information Entropy</a></li>
                </ol>

                <h5 id="intro">Section 2.1 - Introduction & Motivation</h5>

				<p>
				    Information theory, aside from being an interesting line of
                    research in itself, represents a profound intersection of
                    disciplines. There are information-theoretic questions and
                    lines of reasoning being used by mathematicians, engineers,
                    theoretical computer scientists, and both theoretical and
                    experimental physicists. For our purposes, we will mainly be
                    looking at portions of interest to theoretical physicists
                    and mathematicians. In this section, we will focus mainly on
                    understanding fundamental results.
				</p>

                <p>
                    For the physicist, information theory has been of monumental
                    interest since the early days of quantum mechanics. Upon
                    discovering the correlations between certain measurements
                    made on quantum states (a phenomenon of great importance
                    called entanglement), basic questions of how information is
                    shared between states have fascinated and beguiled the 
                    community. Furthermore, when coming to grips with
                    entanglement and its implications, physicists began to
                    wonder how it might be used in a practical setting: and so
                    began the field of study we now know as quantum computing.
                    Those physicists with a bent towards the theoretical
                    continued to wonder at the basic nature of information, and
                    assert the maxim that information itself if a physical
                    quantity to be measured  - that it, too, with mass and
                    charge and all the rest is a quantity by which we might
                    characterise physical phenomena.
                </p>

                <p>
                    For many, information theory as a field of research starts
                    with Claude Shannon's landmark paper, 
                    <a href ="https://culturemath.ens.fr/sites/default/files/p3-shannon.pdf">
                    "A Mathematical Theory of Communication"</a>, first
                    published in the Bell Systems Journal in 1948. In it,
                    Shannon posits some very reasonable properties about
                    signals coming from random source and explores the
                    consequences of these properties.  Starting with the case
                    of discrete noiseless sources and eventually generalizing
                    first to discrete noisy sources and eventually to
                    continuous signals. So many foundational results were
                    described in this paper that much of this article will be
                    devoted simply to its explanation.
                </p>

                <p>
                    We start with a description of what we call a "communication
                    system". This is something best described by a diagram. 
                    <br> 
                    Here is a diagram.
                    
                    <img src="com-scheme.png" alt="Diagram of a communication
                    system" class="center">

                </p>

                <p>
                    In its simplest form, this is what signal-based
                    communication looks like. As an example, say we're talking
                    about landline telephones. Starting from the speaker, the
                    sound reverberates on a diaphragm attached to some iron and
                    placed near a magnet. The motion of the magnetized  iron
                    induces an electric current which is transfered over wiring
                    to an operator. The operator on a switchboard then connects
                    the speaker to the intended recipient, whose own device
                    performs the reverse process to decode the message - turning
                    the recieved current into reverberation identical (up to
                    some noise) to that produced by the speaker. At just about
                    any of these points there is a capacty for noise to be
                    introduced - either from sheer loss in fidelity from the
                    equipment, or from intentional interference by say, the
                    operator in this example. 
                </p>
                
                <p>
                    Now this is stripped down enough to be generalizable, but
                    that means for this to be useful we need to consider cases
                    with their own structure. The first such case we'll be
                    exploring is the simplest, and it's where Shannon starts
                    too: the discrete, noiseless channel. Discrete, noiseless
                    channels are communication schemes where the messages are
                    sent and received as discrete signals, and there is no
                    interference from the outside or within the equipment - an
                    idealized case, to be sure! Near approximations are
                    very well-designed telegraph machines, or shipboard signal
                    flags on a clear day. We also assume that the messages are
                    encoded with a finite character set - as in both of the
                    examples above.
                </p>


                <h5 id="capacity">Section 2.2 - Channel Capacity</h5>

                <p>
                    Shannon begins in a natural place in the description of such
                    schemes: with the capacity of the scheme itself. This
                    quantity is defined in the following way:

                    $$
                        C = \lim_{T \to \infty} \frac{\log_2(N(T)}{T},
                    $$

                    where $N(T)$ is the number of number of allowed signals of
                    duration $T$. This definition works for the discrete
                    noiseless channel, but needs to be generalized for other
                    cases. For polynomial $N(T)$ of order less than $2^{T^2}$,
                    we will have a finite channel capacity, as we will for any
                    function bounded in size by $2^{T^2}$ from above. We have a
                    vacuous lower bound of signals of length 0.
                </p>

                <p>
                    Messages transferred may be encoded in a character set of
                    smaller size than the set used in the original alphabet that
                    constructed the message, e.g. we may encode the latin
                    alphabet into the 32 bitstrings of length 5, with characters
                    to spare. If we imagine this kind of encoding, we may also
                    imagine a graph that represents the construction of these
                    strings. Starting from an empty string, we may either append
                    a 1 or a 0 to it, and repeat for as long as we wish to make
                    our message. We may also encode seperation strings, things
                    that specify that what message being sent is a letter or a
                    word, sending us back to a new empty string. We thus have
                    two nodes: one for beginning new strings and one for 
                    building them. This sort of messaging system, in which there
                    are only a few distinct options after each step, are in fact
                    excellent models of practical communication systems. So what
                    is the capacity of such a system?
                </p>

                <p> We will prove that is comes down to the following. <br>
                    <b>THEOREM 1:</b> Let $b^{(s)}_{ij}$ be the duration of an
                    allowable symbol $s$ that leads from state $i$ to state
                    $j$. Then the channel capacity $C$ is equal to $\log W$,
                    there $W$ is given by the largest real solution to the
                    determinant equation
                    $$
                    \left| \sum_s W^{-b^{s}_{ij}} - \delta_{ij} \right| = 0.
                    $$
                    <br>
                    <b>PROOF:</b>Let $N_i(L)$ be the number of strings of length
                    L that end in state $i$. We may write that 
                    $$N_j(L) = \sum_{i,s} N_i(L - b_{i,j}^{(s)}$$
                    where $b_{i,j}^{(n)}$ are the lengths of the symbols which
                    take state $i$ to state $j$. The objects being summed over
                    are linear differences and thus we have a solution of the
                    form $N_j = A_jW^L$ as $L \rightarrow \infty$. Substituting
                    in this solution form into our difference equation, we
                    recover
                    $$
                    A_jW^L = \sum_{i,s}A_iW^{L-b_{ij}^{(s)}}\\
                    A_j = \sum_{i,s}A_iW^{-b_{ij}^{(s)}}\\ 
                    \sum_i\left(\sum_s W^{-b_{ij}^{(s)}}-\delta_{ij}\right)A_i 
                    = 0.
                    $$
                    For this to make sense, we need that the determinant
                    vanishes: 
                    $$D(W) = \left|a_{ij}\right| = 
                    \left| \sum_s W^{-b^{s}_{ij}} - \delta_{ij} \right| = 0$$.
                    This requirement uniquely characterises $W$, and it is the
                    largest root to the determinant equation.$\blacksquare$ <br>
                    We may then give the quantity $C$ as
                    $$
                    C = \lim_{L \to \infty}\frac{\log\sum_j{A_jW^L}}{L} = \log W.
                    $$
                    A more generalised definition of the channel capacity is
                    $C = \sup_{p_X(x)} I(X;Y)$, where $I(X;Y)$ is the mutual
                    information shared by random variables $X,Y$ and the
                    supremum is taken over the marginal distribution $p_X(x)$
                    which happens to characterise the joint distribution between
                    $X$ and $Y$. We choose to ignore this definition for now, as
                    it's needless but bears mentioning. 
                </p>

                <h5 id="markov">Section 2.3 - Markov and Ergodic Processes</h5>

                <p>
                    We often come across information systems that produce
                    seemingly random outputs across a generally non-uniform
                    distribution. We call strings generated by these information
                    systems "Markov chains" (also sometimes called
                    "Markov processes"). In a Markov process, the proceeding
                    steps may be influenced by the position in which one
                    starts. We call this re-adjustment of distributions a
                    "residue of influence". Markov processes are another
                    construction that are well expressed graphically. 

                    <img src="markov.png" alt="Diagram of a Markov process"
                    class="center">

                    In this example, we can see that the distribution associated
                    with this process is going to produce strings with many C's.
                </p>

                <p>
                    There is extensive literature on the application of Markov
                    processes in statistics, but we're going to focus on a
                    particular type of stochastic process called an "ergodic
                    process". Ergodic processes are those which have the
                    property that their statistical natures may be determined
                    from a single, sufficiently long random sampling. In plain
                    language, that means that each sequence produced by an
                    ergodic process has the same kind of statistical properties
                     - individual letter distributions, digram and trigram
                     distributions, and so on. Resultant of this definition is
                     an inference that we can make on the structure of the graph
                     defining such a process: for one, the graphs must not
                     contain "islands" that are impossible to escape
                </p>
                    
                <h5 id="entropy">Section 2.4 - Information Entropy</h5>

            </div>
        </div>
        <!-- /post -->
    </div>
</div>

	<footer>
		<div class="container">
			<div class="units-row">
			    <div class="unit-100">
					<ul class="social list-flat right">
						<li><a href="mailto:will.gertler@gmail.com"><i class="fa fa-send"></i></a></li>
						<li><a href="http://twitter.com/swickrotation"><i class="fa fa-twitter"></i></a></li>
						<li><a href="https://www.linkedin.com/in/william-gertler-7a965375/"><i class="fa fa-linkedin"></i></a></li>
					</ul>
				</div>
			</div>
		</div>
	</footer>

	<!-- Javascript -->
	<script src="/js/jquery.js"></script>
    <script src="/js/kube.js"></script>
</body>
</html>
